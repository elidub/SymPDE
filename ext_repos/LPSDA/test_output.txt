dataloader shapes 1 1 1
Training on dataset data/KdV_train_10_easy.h5
models/FNO1d_KdV_samples10_augmentation0010_shiftfourier_future20_time11241552.pt
Number of parameters: 10856084
Epoch 0
Starting epoch 0...
/home/eliasd/.conda/envs/sympde/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Training Loss (progress: 0): 1586.340576171875
Training Loss (progress: 1): 1525.48876953125
Training Loss (progress: 2): 1496.8935546875
Training Loss (progress: 3): 1559.850341796875
Training Loss (progress: 4): 1603.039306640625
Training Loss (progress: 5): 1592.9698486328125
Training Loss (progress: 6): 1622.060546875
Training Loss (progress: 7): 1604.523193359375
Training Loss (progress: 8): 1573.7672119140625
Training Loss (progress: 9): 1625.3438720703125
Training Loss (progress: 10): 1589.927001953125
Training Loss (progress: 11): 1543.87158203125
Training Loss (progress: 12): 1539.990478515625
Training Loss (progress: 13): 1537.3995361328125
Training Loss (progress: 14): 1467.574462890625
Training Loss (progress: 15): 1527.482177734375
Training Loss (progress: 16): 1565.27099609375
Training Loss (progress: 17): 1558.663330078125
Training Loss (progress: 18): 1555.3935546875
Training Loss (progress: 19): 1606.64990234375
Training Loss (progress: 20): 1476.49755859375
Training Loss (progress: 21): 1643.2958984375
Training Loss (progress: 22): 1567.826904296875
Training Loss (progress: 23): 1507.355712890625
Training Loss (progress: 24): 1511.86083984375
Training Loss (progress: 25): 1533.9342041015625
Training Loss (progress: 26): 1580.015869140625
Training Loss (progress: 27): 1619.6728515625
Training Loss (progress: 28): 1522.4176025390625
Training Loss (progress: 29): 1501.1614990234375
Training Loss (progress: 30): 1529.7578125
Training Loss (progress: 31): 1527.172607421875
Training Loss (progress: 32): 1558.41015625
Training Loss (progress: 33): 1414.662353515625
Training Loss (progress: 34): 1563.442138671875
Training Loss (progress: 35): 1588.55126953125
Training Loss (progress: 36): 1498.318115234375
Training Loss (progress: 37): 1378.756103515625
Training Loss (progress: 38): 1490.614013671875
Training Loss (progress: 39): 1414.236083984375
Training Loss (progress: 40): 1549.075927734375
Training Loss (progress: 41): 1351.584228515625
Training Loss (progress: 42): 1324.182373046875
Training Loss (progress: 43): 1268.714599609375
Training Loss (progress: 44): 1235.891357421875
Training Loss (progress: 45): 1207.38720703125
Training Loss (progress: 46): 1210.3564453125
Training Loss (progress: 47): 1253.89697265625
Training Loss (progress: 48): 1151.9837646484375
Training Loss (progress: 49): 1153.830322265625
Training Loss (progress: 50): 1019.8786010742188
Training Loss (progress: 51): 970.490478515625
Training Loss (progress: 52): 997.2058715820312
Training Loss (progress: 53): 957.4169921875
Training Loss (progress: 54): 1010.91845703125
Training Loss (progress: 55): 915.1910400390625
Training Loss (progress: 56): 703.394287109375
Training Loss (progress: 57): 863.3839721679688
Training Loss (progress: 58): 672.197021484375
Training Loss (progress: 59): 789.275146484375
Training Loss (progress: 60): 689.2850341796875
Training Loss (progress: 61): 732.1292724609375
Training Loss (progress: 62): 685.4901123046875
Training Loss (progress: 63): 655.4727783203125
Training Loss (progress: 64): 691.6986083984375
Training Loss (progress: 65): 678.3007202148438
Training Loss (progress: 66): 791.7315673828125
Training Loss (progress: 67): 606.0362548828125
Training Loss (progress: 68): 657.928466796875
Training Loss (progress: 69): 537.1234130859375
Training Loss (progress: 70): 629.467529296875
Training Loss (progress: 71): 572.701171875
Training Loss (progress: 72): 558.0723876953125
Training Loss (progress: 73): 478.60528564453125
Training Loss (progress: 74): 463.5316467285156
Training Loss (progress: 75): 495.17816162109375
Training Loss (progress: 76): 607.74755859375
Training Loss (progress: 77): 458.6159362792969
Training Loss (progress: 78): 368.1417236328125
Training Loss (progress: 79): 533.910888671875
Training Loss (progress: 80): 473.456298828125
Training Loss (progress: 81): 450.03936767578125
Training Loss (progress: 82): 473.46453857421875
Training Loss (progress: 83): 440.23565673828125
Training Loss (progress: 84): 332.1700134277344
Training Loss (progress: 85): 295.90802001953125
Training Loss (progress: 86): 354.98931884765625
Training Loss (progress: 87): 311.09625244140625
Training Loss (progress: 88): 321.9322204589844
Training Loss (progress: 89): 338.7568359375
Training Loss (progress: 90): 373.0950927734375
Training Loss (progress: 91): 310.00244140625
Training Loss (progress: 92): 262.25
Training Loss (progress: 93): 274.9376220703125
Training Loss (progress: 94): 270.36968994140625
Training Loss (progress: 95): 276.9306640625
Training Loss (progress: 96): 203.49786376953125
Training Loss (progress: 97): 242.67762756347656
Training Loss (progress: 98): 190.3683319091797
Training Loss (progress: 99): 230.42138671875
Training Loss (progress: 100): 205.60128784179688
Training Loss (progress: 101): 223.96078491210938
Training Loss (progress: 102): 211.59817504882812
Training Loss (progress: 103): 199.710205078125
Training Loss (progress: 104): 177.24681091308594
Training Loss (progress: 105): 186.2063751220703
Training Loss (progress: 106): 189.192626953125
Training Loss (progress: 107): 228.49354553222656
Training Loss (progress: 108): 207.50314331054688
Training Loss (progress: 109): 222.1681671142578
Training Loss (progress: 110): 191.24266052246094
Training Loss (progress: 111): 177.6025848388672
Training Loss (progress: 112): 176.23236083984375
Training Loss (progress: 113): 150.0505828857422
Training Loss (progress: 114): 135.9988250732422
Training Loss (progress: 115): 173.04217529296875
Training Loss (progress: 116): 153.7193603515625
Training Loss (progress: 117): 138.87063598632812
Training Loss (progress: 118): 155.95721435546875
Training Loss (progress: 119): 137.44345092773438
Training Loss (progress: 120): 170.75701904296875
Training Loss (progress: 121): 145.37625122070312
Training Loss (progress: 122): 137.6947021484375
Training Loss (progress: 123): 146.47463989257812
Training Loss (progress: 124): 136.16134643554688
Training Loss (progress: 125): 169.61993408203125
Training Loss (progress: 126): 113.98287963867188
Training Loss (progress: 127): 156.8208465576172
Training Loss (progress: 128): 118.85255432128906
Training Loss (progress: 129): 131.9287109375
Training Loss (progress: 130): 143.17825317382812
Training Loss (progress: 131): 130.28497314453125
Training Loss (progress: 132): 119.91574096679688
Training Loss (progress: 133): 116.55599212646484
Training Loss (progress: 134): 124.85224914550781
Training Loss (progress: 135): 118.05867767333984
Training Loss (progress: 136): 112.82575988769531
Training Loss (progress: 137): 112.59772491455078
Training Loss (progress: 138): 114.31315612792969
Training Loss (progress: 139): 141.96490478515625
Training Loss (progress: 140): 113.58136749267578
Training Loss (progress: 141): 98.49317932128906
Training Loss (progress: 142): 103.99510192871094
Training Loss (progress: 143): 120.46466064453125
Training Loss (progress: 144): 120.05998229980469
Training Loss (progress: 145): 92.49020385742188
Training Loss (progress: 146): 113.84529113769531
Training Loss (progress: 147): 110.29244995117188
Training Loss (progress: 148): 99.23178100585938
Training Loss (progress: 149): 102.32203674316406
Training Loss (progress: 150): 99.6498031616211
Training Loss (progress: 151): 97.49214172363281
Training Loss (progress: 152): 99.7095947265625
Training Loss (progress: 153): 109.78572845458984
Training Loss (progress: 154): 93.26040649414062
Training Loss (progress: 155): 95.16192626953125
Training Loss (progress: 156): 104.92684936523438
Training Loss (progress: 157): 96.12318420410156
Training Loss (progress: 158): 119.20585632324219
Training Loss (progress: 159): 108.20291900634766
Training Loss (progress: 160): 97.63087463378906
Training Loss (progress: 161): 103.35018920898438
Training Loss (progress: 162): 76.14146423339844
Training Loss (progress: 163): 113.15133666992188
Training Loss (progress: 164): 90.93759155273438
Training Loss (progress: 165): 92.19569396972656
Training Loss (progress: 166): 94.58522033691406
Training Loss (progress: 167): 78.94554138183594
Training Loss (progress: 168): 89.24779510498047
Training Loss (progress: 169): 80.16415405273438
Training Loss (progress: 170): 82.69467163085938
Training Loss (progress: 171): 95.2317123413086
Training Loss (progress: 172): 70.02653503417969
Training Loss (progress: 173): 77.10016632080078
Training Loss (progress: 174): 81.2533950805664
Training Loss (progress: 175): 108.59553527832031
Training Loss (progress: 176): 95.82049560546875
Training Loss (progress: 177): 89.84980773925781
Training Loss (progress: 178): 91.17068481445312
Training Loss (progress: 179): 78.90096282958984
Training Loss (progress: 180): 67.75997161865234
Training Loss (progress: 181): 70.79722595214844
Training Loss (progress: 182): 85.37602233886719
Training Loss (progress: 183): 85.59686279296875
Training Loss (progress: 184): 69.56956481933594
Training Loss (progress: 185): 94.2086410522461
Training Loss (progress: 186): 73.46927642822266
Training Loss (progress: 187): 61.15656280517578
Training Loss (progress: 188): 74.58242797851562
Training Loss (progress: 189): 94.54039764404297
Training Loss (progress: 190): 82.20169067382812
Training Loss (progress: 191): 59.5529899597168
Training Loss (progress: 192): 67.737060546875
Training Loss (progress: 193): 85.31998443603516
Training Loss (progress: 194): 64.31161499023438
Training Loss (progress: 195): 52.09095001220703
Training Loss (progress: 196): 86.61155700683594
Training Loss (progress: 197): 77.20635223388672
Training Loss (progress: 198): 66.49563598632812
Training Loss (progress: 199): 73.18775939941406
Training Loss (progress: 200): 72.46507263183594
Training Loss (progress: 201): 65.6850814819336
Training Loss (progress: 202): 56.55415725708008
Training Loss (progress: 203): 65.45844268798828
Training Loss (progress: 204): 61.757076263427734
Training Loss (progress: 205): 73.07070922851562
Training Loss (progress: 206): 55.23640823364258
Training Loss (progress: 207): 65.39713287353516
Training Loss (progress: 208): 63.86978530883789
Training Loss (progress: 209): 73.60468292236328
Training Loss (progress: 210): 62.501983642578125
Training Loss (progress: 211): 47.724395751953125
Training Loss (progress: 212): 55.149864196777344
Training Loss (progress: 213): 43.73283767700195
Training Loss (progress: 214): 51.190895080566406
Training Loss (progress: 215): 50.5474853515625
Training Loss (progress: 216): 56.167137145996094
Training Loss (progress: 217): 67.89231872558594
Training Loss (progress: 218): 46.761863708496094
Training Loss (progress: 219): 54.06949234008789
Training Loss (progress: 220): 77.55352783203125
Training Loss (progress: 221): 73.84251403808594
Training Loss (progress: 222): 61.34953689575195
Training Loss (progress: 223): 55.62205505371094
Training Loss (progress: 224): 50.15057373046875
Training Loss (progress: 225): 61.420230865478516
Training Loss (progress: 226): 57.92948913574219
Training Loss (progress: 227): 53.58698272705078
Training Loss (progress: 228): 53.658058166503906
Training Loss (progress: 229): 70.98838806152344
Training Loss (progress: 230): 54.842586517333984
Training Loss (progress: 231): 43.291255950927734
Training Loss (progress: 232): 46.57283401489258
Training Loss (progress: 233): 51.905426025390625
Training Loss (progress: 234): 45.85137939453125
Training Loss (progress: 235): 44.9703369140625
Training Loss (progress: 236): 54.26392364501953
Training Loss (progress: 237): 46.01033020019531
Training Loss (progress: 238): 46.22492980957031
Training Loss (progress: 239): 44.50162887573242
Training Loss (progress: 240): 57.22050094604492
Training Loss (progress: 241): 47.976280212402344
Training Loss (progress: 242): 46.498374938964844
Training Loss (progress: 243): 51.040287017822266
Training Loss (progress: 244): 57.424049377441406
Training Loss (progress: 245): 50.63553237915039
Training Loss (progress: 246): 38.89425277709961
Training Loss (progress: 247): 47.13738250732422
Training Loss (progress: 248): 58.74824905395508
Training Loss (progress: 249): 48.30696105957031
Training Loss (progress: 250): 52.5738525390625
Training Loss (progress: 251): 53.358428955078125
Training Loss (progress: 252): 47.3160514831543
Training Loss (progress: 253): 36.1866455078125
Training Loss (progress: 254): 32.227012634277344
Training Loss (progress: 255): 39.07649230957031
Training Loss (progress: 256): 38.81230926513672
Training Loss (progress: 257): 42.98645782470703
Training Loss (progress: 258): 43.273414611816406
Training Loss (progress: 259): 38.02684020996094
Training Loss (progress: 260): 45.81941604614258
Training Loss (progress: 261): 40.92457580566406
Training Loss (progress: 262): 39.8928337097168
Training Loss (progress: 263): 40.66392517089844
Training Loss (progress: 264): 44.852027893066406
Training Loss (progress: 265): 28.364673614501953
Training Loss (progress: 266): 60.830841064453125
Training Loss (progress: 267): 38.469886779785156
Training Loss (progress: 268): 37.713871002197266
Training Loss (progress: 269): 47.01795959472656
Training Loss (progress: 270): 32.44417190551758
Training Loss (progress: 271): 38.23403549194336
Training Loss (progress: 272): 37.84672546386719
Training Loss (progress: 273): 38.648170471191406
Training Loss (progress: 274): 41.60282897949219
Training Loss (progress: 275): 29.877525329589844
Training Loss (progress: 276): 43.178855895996094
Training Loss (progress: 277): 39.55632019042969
Training Loss (progress: 278): 37.88528060913086
Training Loss (progress: 279): 42.48133087158203
Evaluation on validation dataset:
/home/eliasd/.conda/envs/sympde/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Input 20 - 40, mean loss 235.34141540527344
Input 40 - 60, mean loss 192.32098388671875
Input 60 - 80, mean loss 197.92681884765625
Input 80 - 100, mean loss 202.99598693847656
Input 100 - 120, mean loss 205.2352752685547
loooes tmp [tensor(0.9193, device='cuda:0'), tensor(2.8232, device='cuda:0'), tensor(4.9215, device='cuda:0'), tensor(7.0620, device='cuda:0'), tensor(8.9494, device='cuda:0')]
Unrolled forward losses: 24.6754 +- 0.0000
Unrolled forward losses (normalized): 34.2942 +- 0.0000
Evaluation on test dataset:
/home/eliasd/.conda/envs/sympde/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Input 20 - 40, mean loss 32.277854919433594
Input 40 - 60, mean loss 37.78968048095703
Input 60 - 80, mean loss 39.84779357910156
Input 80 - 100, mean loss 35.8085823059082
Input 100 - 120, mean loss 33.703941345214844
loooes tmp [tensor(0.1261, device='cuda:0'), tensor(0.5792, device='cuda:0'), tensor(1.3004, device='cuda:0'), tensor(2.0865, device='cuda:0'), tensor(2.7436, device='cuda:0')]
Unrolled forward losses: 6.8357 +- 0.0000
Unrolled forward losses (normalized): 23.5961 +- 0.0000
Saved model at models/FNO1d_KdV_samples10_augmentation0010_shiftfourier_future20_time11241552.pt

Epoch 1
Starting epoch 1...
Training Loss (progress: 0): 50.52648162841797
Training Loss (progress: 1): 30.18467140197754
Training Loss (progress: 2): 37.79558181762695
Training Loss (progress: 3): 28.083114624023438
Training Loss (progress: 4): 31.204715728759766
Training Loss (progress: 5): 40.241146087646484
Training Loss (progress: 6): 35.477813720703125
Training Loss (progress: 7): 30.713577270507812
Training Loss (progress: 8): 36.409263610839844
Training Loss (progress: 9): 34.82129669189453
Training Loss (progress: 10): 35.26847839355469
Training Loss (progress: 11): 43.794776916503906
Training Loss (progress: 12): 33.01567077636719
Training Loss (progress: 13): 38.22831726074219
Training Loss (progress: 14): 31.22524642944336
Training Loss (progress: 15): 29.237014770507812
Training Loss (progress: 16): 39.4919548034668
Training Loss (progress: 17): 33.06359100341797
Training Loss (progress: 18): 27.042707443237305
Training Loss (progress: 19): 28.784988403320312
Training Loss (progress: 20): 31.195178985595703
Training Loss (progress: 21): 27.61101531982422
Training Loss (progress: 22): 40.67003631591797
Training Loss (progress: 23): 32.72428894042969
Training Loss (progress: 24): 28.361650466918945
Training Loss (progress: 25): 33.34214782714844
Training Loss (progress: 26): 40.45232391357422
Training Loss (progress: 27): 42.3181266784668
Training Loss (progress: 28): 33.70860290527344
Training Loss (progress: 29): 32.83885955810547
Training Loss (progress: 30): 33.038970947265625
Training Loss (progress: 31): 29.000234603881836
Training Loss (progress: 32): 35.48554992675781
Training Loss (progress: 33): 23.535781860351562
Training Loss (progress: 34): 28.065488815307617
Training Loss (progress: 35): 32.16936492919922
Training Loss (progress: 36): 25.904769897460938
Training Loss (progress: 37): 36.889137268066406
Training Loss (progress: 38): 27.87897491455078
Training Loss (progress: 39): 26.90517234802246
Training Loss (progress: 40): 30.229965209960938
Training Loss (progress: 41): 31.490184783935547
Training Loss (progress: 42): 30.892990112304688
Training Loss (progress: 43): 28.993772506713867
Training Loss (progress: 44): 24.77557373046875
Training Loss (progress: 45): 34.461761474609375
Training Loss (progress: 46): 32.61741638183594
Training Loss (progress: 47): 23.860565185546875
Training Loss (progress: 48): 23.448589324951172
Training Loss (progress: 49): 45.149269104003906
Training Loss (progress: 50): 24.113788604736328
Training Loss (progress: 51): 29.52541160583496
Training Loss (progress: 52): 30.150230407714844
Training Loss (progress: 53): 31.321044921875
Training Loss (progress: 54): 23.04245376586914
Training Loss (progress: 55): 25.792526245117188
Training Loss (progress: 56): 32.71388244628906
Training Loss (progress: 57): 30.976856231689453
Training Loss (progress: 58): 32.59587478637695
Training Loss (progress: 59): 31.52295684814453
Training Loss (progress: 60): 23.849029541015625
Training Loss (progress: 61): 30.731727600097656
Training Loss (progress: 62): 28.934917449951172
Training Loss (progress: 63): 19.901172637939453
Training Loss (progress: 64): 22.344907760620117
Training Loss (progress: 65): 42.62089538574219
Training Loss (progress: 66): 33.59648132324219
Training Loss (progress: 67): 32.82470703125
Training Loss (progress: 68): 23.51177978515625
Training Loss (progress: 69): 32.06977462768555
Training Loss (progress: 70): 21.445619583129883
Training Loss (progress: 71): 26.28833770751953
Training Loss (progress: 72): 29.42833709716797
Training Loss (progress: 73): 22.072038650512695
Training Loss (progress: 74): 22.212501525878906
Training Loss (progress: 75): 28.403488159179688
Training Loss (progress: 76): 30.769731521606445
Training Loss (progress: 77): 31.20701789855957
Training Loss (progress: 78): 18.080886840820312
Training Loss (progress: 79): 20.719165802001953
Training Loss (progress: 80): 21.634716033935547
Training Loss (progress: 81): 47.1740608215332
Training Loss (progress: 82): 19.825904846191406
Training Loss (progress: 83): 21.6285343170166
Training Loss (progress: 84): 28.915782928466797
Training Loss (progress: 85): 23.02527618408203
Training Loss (progress: 86): 24.411130905151367
Training Loss (progress: 87): 19.847396850585938
Training Loss (progress: 88): 31.829423904418945
Training Loss (progress: 89): 21.899932861328125
Training Loss (progress: 90): 43.89105987548828
Training Loss (progress: 91): 19.34598159790039
Training Loss (progress: 92): 36.38951110839844
Training Loss (progress: 93): 25.20489501953125
Training Loss (progress: 94): 26.304311752319336
Training Loss (progress: 95): 30.19037628173828
Training Loss (progress: 96): 20.217758178710938
Training Loss (progress: 97): 21.225860595703125
Training Loss (progress: 98): 25.167509078979492
Training Loss (progress: 99): 29.613874435424805
Training Loss (progress: 100): 23.52060890197754
Training Loss (progress: 101): 20.063905715942383
Training Loss (progress: 102): 18.004165649414062
Training Loss (progress: 103): 26.22842788696289
Training Loss (progress: 104): 19.103382110595703
Training Loss (progress: 105): 19.16720199584961
Training Loss (progress: 106): 27.66062355041504
Training Loss (progress: 107): 20.378047943115234
Training Loss (progress: 108): 25.76689910888672
Training Loss (progress: 109): 20.721166610717773
Training Loss (progress: 110): 25.212162017822266
Training Loss (progress: 111): 22.128543853759766
Training Loss (progress: 112): 19.606060028076172
Training Loss (progress: 113): 19.414623260498047
Training Loss (progress: 114): 20.750080108642578
Training Loss (progress: 115): 24.327842712402344
Training Loss (progress: 116): 23.253450393676758
Training Loss (progress: 117): 22.41407585144043
Training Loss (progress: 118): 26.05258560180664
Training Loss (progress: 119): 22.450014114379883
Training Loss (progress: 120): 25.732906341552734
Training Loss (progress: 121): 16.767242431640625
Training Loss (progress: 122): 23.288663864135742
Training Loss (progress: 123): 26.650005340576172
Training Loss (progress: 124): 20.03062629699707
Training Loss (progress: 125): 18.763103485107422
Training Loss (progress: 126): 17.26025390625
Training Loss (progress: 127): 19.00484848022461
Training Loss (progress: 128): 21.41021728515625
Training Loss (progress: 129): 18.742246627807617
Training Loss (progress: 130): 20.777713775634766
Training Loss (progress: 131): 22.857196807861328
Training Loss (progress: 132): 22.537006378173828
Training Loss (progress: 133): 19.834941864013672
Training Loss (progress: 134): 18.359561920166016
Training Loss (progress: 135): 17.080142974853516
Training Loss (progress: 136): 29.683185577392578
Training Loss (progress: 137): 25.512781143188477
Training Loss (progress: 138): 22.695205688476562
Training Loss (progress: 139): 18.66525650024414
Training Loss (progress: 140): 21.062421798706055
Training Loss (progress: 141): 20.27794075012207
Training Loss (progress: 142): 23.504350662231445
Training Loss (progress: 143): 18.85306739807129
Training Loss (progress: 144): 18.596033096313477
Training Loss (progress: 145): 15.345390319824219
Training Loss (progress: 146): 17.84131622314453
Training Loss (progress: 147): 21.97777557373047
Training Loss (progress: 148): 18.411388397216797
Training Loss (progress: 149): 18.223421096801758
Training Loss (progress: 150): 15.921411514282227
Training Loss (progress: 151): 19.406278610229492
Training Loss (progress: 152): 14.414258003234863
Training Loss (progress: 153): 19.433399200439453
Training Loss (progress: 154): 15.703207969665527
Training Loss (progress: 155): 38.4111442565918
Training Loss (progress: 156): 16.326629638671875
Training Loss (progress: 157): 16.275888442993164
Training Loss (progress: 158): 23.055606842041016
Training Loss (progress: 159): 19.386619567871094
Training Loss (progress: 160): 18.600418090820312
Training Loss (progress: 161): 16.671646118164062
Training Loss (progress: 162): 17.215478897094727
Training Loss (progress: 163): 22.378189086914062
Training Loss (progress: 164): 23.87563705444336
Training Loss (progress: 165): 18.474164962768555
Training Loss (progress: 166): 20.563623428344727
Training Loss (progress: 167): 21.759971618652344
Training Loss (progress: 168): 16.8558349609375
Training Loss (progress: 169): 17.92003631591797
Training Loss (progress: 170): 18.469453811645508
Training Loss (progress: 171): 15.479667663574219
Training Loss (progress: 172): 15.243237495422363
Training Loss (progress: 173): 20.538978576660156
Training Loss (progress: 174): 23.031036376953125
Training Loss (progress: 175): 31.51274871826172
Training Loss (progress: 176): 23.41971206665039
Training Loss (progress: 177): 14.249897003173828
Training Loss (progress: 178): 16.939002990722656
Training Loss (progress: 179): 23.36121368408203
Training Loss (progress: 180): 17.726924896240234
Training Loss (progress: 181): 17.362064361572266
Training Loss (progress: 182): 18.289525985717773
Training Loss (progress: 183): 17.387691497802734
Training Loss (progress: 184): 21.18159294128418
Training Loss (progress: 185): 17.52290916442871
Training Loss (progress: 186): 16.29230499267578
Training Loss (progress: 187): 30.03326988220215
Training Loss (progress: 188): 17.3914852142334
Training Loss (progress: 189): 17.879318237304688
Training Loss (progress: 190): 16.82644271850586
Training Loss (progress: 191): 20.127609252929688
Training Loss (progress: 192): 18.53215980529785
Training Loss (progress: 193): 23.250423431396484
Training Loss (progress: 194): 15.92617130279541
Training Loss (progress: 195): 21.593894958496094
Training Loss (progress: 196): 19.171934127807617
Training Loss (progress: 197): 14.997419357299805
Training Loss (progress: 198): 18.355138778686523
Training Loss (progress: 199): 17.97606658935547
Training Loss (progress: 200): 15.625741958618164
Training Loss (progress: 201): 25.294565200805664
Training Loss (progress: 202): 19.71035385131836
Training Loss (progress: 203): 15.388001441955566
Training Loss (progress: 204): 19.13253402709961
Training Loss (progress: 205): 25.74404525756836
Training Loss (progress: 206): 14.13648796081543
Training Loss (progress: 207): 15.978828430175781
Training Loss (progress: 208): 11.535545349121094
Training Loss (progress: 209): 13.260269165039062
Training Loss (progress: 210): 18.319345474243164
Training Loss (progress: 211): 15.214588165283203
Training Loss (progress: 212): 15.335521697998047
Training Loss (progress: 213): 13.27553939819336
Training Loss (progress: 214): 21.277366638183594
Training Loss (progress: 215): 17.081165313720703
Training Loss (progress: 216): 28.219074249267578
Training Loss (progress: 217): 12.273517608642578
Training Loss (progress: 218): 18.197690963745117
Training Loss (progress: 219): 17.221891403198242
Training Loss (progress: 220): 17.502031326293945
Training Loss (progress: 221): 16.082082748413086
Training Loss (progress: 222): 12.201202392578125
Training Loss (progress: 223): 10.618051528930664
Training Loss (progress: 224): 20.946352005004883
Training Loss (progress: 225): 18.378883361816406
Training Loss (progress: 226): 14.93565559387207
Training Loss (progress: 227): 13.30743408203125
Training Loss (progress: 228): 9.918693542480469
Training Loss (progress: 229): 15.536018371582031
Training Loss (progress: 230): 13.043487548828125
Training Loss (progress: 231): 14.331880569458008
Training Loss (progress: 232): 12.040790557861328
Training Loss (progress: 233): 24.5494384765625
Training Loss (progress: 234): 23.097103118896484
Training Loss (progress: 235): 16.063108444213867
Training Loss (progress: 236): 19.20071792602539
Training Loss (progress: 237): 12.429574966430664
Training Loss (progress: 238): 26.75287628173828
Training Loss (progress: 239): 14.675355911254883
Training Loss (progress: 240): 12.654632568359375
Training Loss (progress: 241): 23.729490280151367
Training Loss (progress: 242): 13.01706314086914
Training Loss (progress: 243): 14.717055320739746
Training Loss (progress: 244): 14.340717315673828
Training Loss (progress: 245): 12.579450607299805
Training Loss (progress: 246): 18.466327667236328
Training Loss (progress: 247): 13.06029987335205
Training Loss (progress: 248): 16.919658660888672
Training Loss (progress: 249): 15.633835792541504
Training Loss (progress: 250): 15.806066513061523
Training Loss (progress: 251): 15.581984519958496
Training Loss (progress: 252): 12.50313949584961
Training Loss (progress: 253): 10.448846817016602
Training Loss (progress: 254): 15.065853118896484
Training Loss (progress: 255): 13.085269927978516
Training Loss (progress: 256): 20.959096908569336
Training Loss (progress: 257): 14.357877731323242
Training Loss (progress: 258): 11.47607421875
Training Loss (progress: 259): 12.363337516784668
Training Loss (progress: 260): 20.587583541870117
Training Loss (progress: 261): 14.313163757324219
Training Loss (progress: 262): 10.431045532226562
Training Loss (progress: 263): 11.458333969116211
Training Loss (progress: 264): 15.134450912475586
Training Loss (progress: 265): 10.631685256958008
Training Loss (progress: 266): 20.03960418701172
Training Loss (progress: 267): 14.37900161743164
Training Loss (progress: 268): 10.189494132995605
Training Loss (progress: 269): 14.308411598205566
Training Loss (progress: 270): 11.878386497497559
Training Loss (progress: 271): 14.574028015136719
Training Loss (progress: 272): 10.421709060668945
Training Loss (progress: 273): 13.53036880493164
Training Loss (progress: 274): 12.898366928100586
Training Loss (progress: 275): 10.96523666381836
Training Loss (progress: 276): 11.144102096557617
Training Loss (progress: 277): 15.973675727844238
Training Loss (progress: 278): 12.195672988891602
Training Loss (progress: 279): 12.180159568786621
Evaluation on validation dataset:
Input 20 - 40, mean loss 161.47775268554688
Input 40 - 60, mean loss 129.303955078125
Input 60 - 80, mean loss 134.28268432617188
Input 80 - 100, mean loss 146.90176391601562
Input 100 - 120, mean loss 162.7909393310547
loooes tmp [tensor(0.6308, device='cuda:0'), tensor(2.1745, device='cuda:0'), tensor(3.8204, device='cuda:0'), tensor(5.6748, device='cuda:0'), tensor(7.2166, device='cuda:0')]
Unrolled forward losses: 19.5170 +- 0.0000
Unrolled forward losses (normalized): 24.3442 +- 0.0000
Evaluation on test dataset:
Input 20 - 40, mean loss 13.709402084350586
Input 40 - 60, mean loss 17.775585174560547
Input 60 - 80, mean loss 21.25865364074707
Input 80 - 100, mean loss 18.802961349487305
Input 100 - 120, mean loss 16.351428985595703
loooes tmp [tensor(0.0536, device='cuda:0'), tensor(0.2603, device='cuda:0'), tensor(0.5777, device='cuda:0'), tensor(0.8954, device='cuda:0'), tensor(1.0994, device='cuda:0')]
Unrolled forward losses: 2.8863 +- 0.0000
Unrolled forward losses (normalized): 10.7009 +- 0.0000
Saved model at models/FNO1d_KdV_samples10_augmentation0010_shiftfourier_future20_time11241552.pt

Epoch 2
Starting epoch 2...
Training Loss (progress: 0): 17.958789825439453
Training Loss (progress: 1): 14.889532089233398
Training Loss (progress: 2): 13.591938018798828
Training Loss (progress: 3): 12.670204162597656
Training Loss (progress: 4): 11.238507270812988
Training Loss (progress: 5): 15.983757019042969
Training Loss (progress: 6): 14.738130569458008
Training Loss (progress: 7): 11.650400161743164
Training Loss (progress: 8): 14.390273094177246
Training Loss (progress: 9): 11.185011863708496
Training Loss (progress: 10): 11.90182876586914
Training Loss (progress: 11): 11.942601203918457
Training Loss (progress: 12): 11.831184387207031
Training Loss (progress: 13): 13.396844863891602
Training Loss (progress: 14): 9.861082077026367
Training Loss (progress: 15): 18.026443481445312
Training Loss (progress: 16): 13.856978416442871
Training Loss (progress: 17): 18.242475509643555
Training Loss (progress: 18): 11.596172332763672
Training Loss (progress: 19): 11.25213623046875
Training Loss (progress: 20): 8.720134735107422
Training Loss (progress: 21): 15.19608211517334
Training Loss (progress: 22): 11.643505096435547
Training Loss (progress: 23): 12.941250801086426
Training Loss (progress: 24): 10.087014198303223
Training Loss (progress: 25): 16.410568237304688
Training Loss (progress: 26): 11.656277656555176
Training Loss (progress: 27): 11.398468017578125
Training Loss (progress: 28): 9.624765396118164
Training Loss (progress: 29): 9.852456092834473
Training Loss (progress: 30): 10.211053848266602
Training Loss (progress: 31): 10.656360626220703
Training Loss (progress: 32): 15.864753723144531
Training Loss (progress: 33): 11.530766487121582
Training Loss (progress: 34): 10.877204895019531
Training Loss (progress: 35): 22.463401794433594
Training Loss (progress: 36): 20.348918914794922
Training Loss (progress: 37): 13.088311195373535
Training Loss (progress: 38): 10.632926940917969
Training Loss (progress: 39): 19.207233428955078
Training Loss (progress: 40): 13.604236602783203
Training Loss (progress: 41): 10.227813720703125
Training Loss (progress: 42): 10.42338752746582
Training Loss (progress: 43): 15.329877853393555
Training Loss (progress: 44): 9.729413986206055
Training Loss (progress: 45): 11.280969619750977
Training Loss (progress: 46): 11.362281799316406
Training Loss (progress: 47): 11.064165115356445
Training Loss (progress: 48): 13.343090057373047
Training Loss (progress: 49): 10.673660278320312
Training Loss (progress: 50): 15.291647911071777
Training Loss (progress: 51): 12.2733736038208
Training Loss (progress: 52): 10.04440689086914
Training Loss (progress: 53): 10.971112251281738
Training Loss (progress: 54): 12.724286079406738
Training Loss (progress: 55): 12.717100143432617
Training Loss (progress: 56): 10.502503395080566
Training Loss (progress: 57): 11.423394203186035
Training Loss (progress: 58): 13.407247543334961
Training Loss (progress: 59): 14.108762741088867
Training Loss (progress: 60): 12.31237506866455
Training Loss (progress: 61): 11.872639656066895
Training Loss (progress: 62): 11.3192720413208
Training Loss (progress: 63): 14.287464141845703
Training Loss (progress: 64): 9.454872131347656
Training Loss (progress: 65): 7.176033973693848
Training Loss (progress: 66): 12.347585678100586
Training Loss (progress: 67): 8.7938232421875
Training Loss (progress: 68): 12.534451484680176
Training Loss (progress: 69): 10.511068344116211
Training Loss (progress: 70): 10.684378623962402
Training Loss (progress: 71): 9.570294380187988
Training Loss (progress: 72): 9.367399215698242
Training Loss (progress: 73): 10.569730758666992
Training Loss (progress: 74): 13.481012344360352
Training Loss (progress: 75): 10.183095932006836
Training Loss (progress: 76): 11.443901062011719
Training Loss (progress: 77): 11.23956298828125
Training Loss (progress: 78): 11.758195877075195
Training Loss (progress: 79): 10.14084243774414
Training Loss (progress: 80): 7.6530537605285645
Training Loss (progress: 81): 7.957212924957275
Training Loss (progress: 82): 12.710334777832031
Training Loss (progress: 83): 10.138447761535645
Training Loss (progress: 84): 15.835638999938965
Training Loss (progress: 85): 8.377016067504883
Training Loss (progress: 86): 8.81903076171875
Training Loss (progress: 87): 13.916526794433594
Training Loss (progress: 88): 10.732322692871094
Training Loss (progress: 89): 9.626779556274414
Training Loss (progress: 90): 14.035040855407715
Training Loss (progress: 91): 14.903655052185059
Training Loss (progress: 92): 15.702865600585938
Training Loss (progress: 93): 16.664470672607422
Training Loss (progress: 94): 14.633869171142578
Training Loss (progress: 95): 12.805068969726562
Training Loss (progress: 96): 14.007318496704102
Training Loss (progress: 97): 12.04253101348877
Training Loss (progress: 98): 12.050191879272461
Training Loss (progress: 99): 11.714078903198242
Training Loss (progress: 100): 9.256878852844238
Training Loss (progress: 101): 11.975690841674805
Training Loss (progress: 102): 14.73362922668457
Training Loss (progress: 103): 10.894355773925781
Training Loss (progress: 104): 10.897431373596191
Training Loss (progress: 105): 10.469388008117676
Training Loss (progress: 106): 11.671299934387207
Training Loss (progress: 107): 10.48373794555664
Training Loss (progress: 108): 9.14908504486084
Training Loss (progress: 109): 9.28290843963623
Training Loss (progress: 110): 7.426851272583008
Training Loss (progress: 111): 10.639568328857422
Training Loss (progress: 112): 9.433862686157227
Training Loss (progress: 113): 12.626522064208984
Training Loss (progress: 114): 11.89525032043457
Training Loss (progress: 115): 9.774198532104492
Training Loss (progress: 116): 7.216876029968262
Training Loss (progress: 117): 9.535307884216309
Training Loss (progress: 118): 7.688666343688965
Training Loss (progress: 119): 10.113523483276367
Training Loss (progress: 120): 9.931381225585938
Training Loss (progress: 121): 10.993045806884766
Training Loss (progress: 122): 12.126075744628906
Training Loss (progress: 123): 10.11058235168457
Training Loss (progress: 124): 13.180225372314453
Training Loss (progress: 125): 9.765384674072266
Training Loss (progress: 126): 8.452512741088867
Training Loss (progress: 127): 15.303730964660645
Training Loss (progress: 128): 7.940554618835449
Training Loss (progress: 129): 10.291091918945312
Training Loss (progress: 130): 7.611842155456543
Training Loss (progress: 131): 12.739251136779785
Training Loss (progress: 132): 10.509310722351074
Training Loss (progress: 133): 7.329174518585205
Training Loss (progress: 134): 8.953683853149414
Training Loss (progress: 135): 8.29231071472168
Training Loss (progress: 136): 8.71005630493164
Training Loss (progress: 137): 7.621111869812012
Training Loss (progress: 138): 11.637748718261719
Training Loss (progress: 139): 10.027522087097168
Training Loss (progress: 140): 7.738267421722412
Training Loss (progress: 141): 9.161581039428711
Training Loss (progress: 142): 8.258106231689453
Training Loss (progress: 143): 8.084633827209473
Training Loss (progress: 144): 7.376895904541016
Training Loss (progress: 145): 9.866804122924805
Training Loss (progress: 146): 8.857059478759766
Training Loss (progress: 147): 7.727124214172363
Training Loss (progress: 148): 7.3542327880859375
Training Loss (progress: 149): 9.51054859161377
Training Loss (progress: 150): 7.076643943786621
Training Loss (progress: 151): 7.431955337524414
Training Loss (progress: 152): 11.552680969238281
Training Loss (progress: 153): 6.626712799072266
Training Loss (progress: 154): 7.2935919761657715
Training Loss (progress: 155): 5.799201011657715
Training Loss (progress: 156): 8.199918746948242
Training Loss (progress: 157): 5.937221527099609
Training Loss (progress: 158): 6.810395240783691
Training Loss (progress: 159): 6.23067045211792
Training Loss (progress: 160): 9.223245620727539
Training Loss (progress: 161): 14.48595905303955
Training Loss (progress: 162): 6.561899185180664
Training Loss (progress: 163): 8.674606323242188
Training Loss (progress: 164): 6.993744373321533
Training Loss (progress: 165): 5.7855119705200195
Training Loss (progress: 166): 7.179388046264648
Training Loss (progress: 167): 10.39391803741455
Training Loss (progress: 168): 6.7648024559021
Training Loss (progress: 169): 7.919440269470215
Training Loss (progress: 170): 5.343677520751953
Training Loss (progress: 171): 6.7025556564331055
Training Loss (progress: 172): 7.759012699127197
Training Loss (progress: 173): 6.531771183013916
Training Loss (progress: 174): 8.327983856201172
Training Loss (progress: 175): 9.483424186706543
Training Loss (progress: 176): 6.844405174255371
Training Loss (progress: 177): 9.247160911560059
Training Loss (progress: 178): 8.211357116699219
Training Loss (progress: 179): 6.206888675689697
Training Loss (progress: 180): 9.136214256286621
Training Loss (progress: 181): 8.825660705566406
Training Loss (progress: 182): 8.682608604431152
Training Loss (progress: 183): 6.98730993270874
Training Loss (progress: 184): 10.243118286132812
Training Loss (progress: 185): 7.817625045776367
Training Loss (progress: 186): 9.883544921875
Training Loss (progress: 187): 10.40943717956543
Training Loss (progress: 188): 7.579975128173828
Training Loss (progress: 189): 10.649974822998047
Training Loss (progress: 190): 12.150768280029297
Training Loss (progress: 191): 7.797756195068359
Training Loss (progress: 192): 5.565680503845215
Training Loss (progress: 193): 12.63066291809082
Training Loss (progress: 194): 6.326473236083984
Training Loss (progress: 195): 6.395506858825684
Training Loss (progress: 196): 7.535210609436035
Training Loss (progress: 197): 7.59905481338501
Training Loss (progress: 198): 11.470306396484375
Training Loss (progress: 199): 8.185267448425293
Training Loss (progress: 200): 9.581939697265625
Training Loss (progress: 201): 7.312419891357422
Training Loss (progress: 202): 9.425701141357422
Training Loss (progress: 203): 6.978102684020996
Training Loss (progress: 204): 7.853450775146484
Training Loss (progress: 205): 6.897858619689941
Training Loss (progress: 206): 7.144742012023926
Training Loss (progress: 207): 7.274913787841797
Training Loss (progress: 208): 9.534772872924805
Training Loss (progress: 209): 11.627901077270508
Training Loss (progress: 210): 9.579533576965332
Training Loss (progress: 211): 6.445638656616211
Training Loss (progress: 212): 6.877684116363525
Training Loss (progress: 213): 10.852712631225586
Training Loss (progress: 214): 6.835933685302734
Training Loss (progress: 215): 7.84237003326416
Training Loss (progress: 216): 8.351418495178223
Training Loss (progress: 217): 13.541125297546387
Training Loss (progress: 218): 6.055588245391846
Training Loss (progress: 219): 9.074682235717773
Training Loss (progress: 220): 11.664350509643555
Training Loss (progress: 221): 6.795063018798828
Training Loss (progress: 222): 8.570483207702637
Training Loss (progress: 223): 6.065793514251709
Training Loss (progress: 224): 8.31985092163086
Training Loss (progress: 225): 7.261273384094238
Training Loss (progress: 226): 7.398378372192383
Training Loss (progress: 227): 8.193413734436035
Training Loss (progress: 228): 7.237293243408203
Training Loss (progress: 229): 6.697611331939697
Training Loss (progress: 230): 8.048099517822266
Training Loss (progress: 231): 7.142017364501953
Training Loss (progress: 232): 7.361483573913574
Training Loss (progress: 233): 7.630195617675781
Training Loss (progress: 234): 8.147882461547852
Training Loss (progress: 235): 6.72331428527832
Training Loss (progress: 236): 7.486064434051514
Training Loss (progress: 237): 6.4704461097717285
Training Loss (progress: 238): 7.21395206451416
Training Loss (progress: 239): 6.57072114944458
Training Loss (progress: 240): 8.154906272888184
Training Loss (progress: 241): 7.220522880554199
Training Loss (progress: 242): 8.238653182983398
Training Loss (progress: 243): 7.010066986083984
Training Loss (progress: 244): 7.4611053466796875
Training Loss (progress: 245): 7.2613019943237305
Training Loss (progress: 246): 8.030864715576172
Training Loss (progress: 247): 6.298625469207764
Training Loss (progress: 248): 6.496586799621582
Training Loss (progress: 249): 9.561201095581055
Training Loss (progress: 250): 5.157627582550049
Training Loss (progress: 251): 10.543028831481934
Training Loss (progress: 252): 5.874086380004883
Training Loss (progress: 253): 6.086461067199707
Training Loss (progress: 254): 6.909122467041016
Training Loss (progress: 255): 8.730269432067871
Training Loss (progress: 256): 5.134520530700684
Training Loss (progress: 257): 6.0208740234375
Training Loss (progress: 258): 7.049324989318848
Training Loss (progress: 259): 5.496053218841553
Training Loss (progress: 260): 7.013795852661133
Training Loss (progress: 261): 5.771243095397949
Training Loss (progress: 262): 6.917477130889893
Training Loss (progress: 263): 6.166333198547363
Training Loss (progress: 264): 7.958944320678711
Training Loss (progress: 265): 6.430549621582031
Training Loss (progress: 266): 5.282709121704102
Training Loss (progress: 267): 10.013084411621094
Training Loss (progress: 268): 7.405255317687988
Traceback (most recent call last):
  File "/gpfs/home6/eliasd/thesis/SymPDE/ext_repos/LPSDA/experiments/train.py", line 396, in <module>
    main(args)
  File "/gpfs/home6/eliasd/thesis/SymPDE/ext_repos/LPSDA/experiments/train.py", line 320, in main
    train(args, pde, epoch, model, optimizer, train_loader, data_creator, criterion, device=device)
  File "/gpfs/home6/eliasd/thesis/SymPDE/ext_repos/LPSDA/experiments/train.py", line 78, in train
    losses = training_loop(pde, model, unrolling, args.batch_size, optimizer, loader, data_creator, criterion, device)
  File "/gpfs/home6/eliasd/thesis/SymPDE/ext_repos/LPSDA/experiments/train_helper.py", line 56, in training_loop
    for (u, dx, dt) in loader:
  File "/home/eliasd/.conda/envs/sympde/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/eliasd/.conda/envs/sympde/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1328, in _next_data
    idx, data = self._get_data()
  File "/home/eliasd/.conda/envs/sympde/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1284, in _get_data
    success, data = self._try_get_data()
  File "/home/eliasd/.conda/envs/sympde/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1132, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/eliasd/.conda/envs/sympde/lib/python3.10/queue.py", line 180, in get
    self.not_empty.wait(remaining)
  File "/home/eliasd/.conda/envs/sympde/lib/python3.10/threading.py", line 324, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt
