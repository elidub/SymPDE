{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_close_flat(a, b):\n",
    "    return torch.allclose(a.flatten(), b.flatten(), atol=1e-6, rtol=1e-6), (a - b)\n",
    "\n",
    "def check(a, b, string):\n",
    "    print(string, a.shape)\n",
    "    print(len(string) * ' ', b.shape)\n",
    "    # all_close_flat(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias = True, padding_mode='zeros'):\n",
    "        super().__init__()\n",
    "        assert padding_mode in ['zeros', 'circular']\n",
    "        if padding_mode == 'zeros':\n",
    "            padding_mode = 'constant'\n",
    "        assert padding == (kernel_size - 1) // 2\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride=stride\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "        self.weights = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size))\n",
    "        self.biases = nn.Parameter(torch.randn(out_channels))\n",
    "\n",
    "    def einsum_alt(self, patches, weights):\n",
    "        patches = patches.permute(0, 2, 1, 3) # # batch_size, width, channels, kernel_size \n",
    "        patches_unsqueezed = patches.unsqueeze(2) # batch_size, 1, width, channels, kernel_size\n",
    "        weights_unsqueezed = weights.unsqueeze(0) # 1, out_channels, in_channels, kernel_size\n",
    "        out = patches_unsqueezed * weights_unsqueezed\n",
    "        out = out.sum([3, 4]) # batch_size, out_channels, width, channels\n",
    "        out = out.permute(0, 2, 1) # batch_size, out_channels, output_pixels\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, in_channels2, width = x.shape\n",
    "        assert in_channels2 == self.in_channels\n",
    "\n",
    "        \n",
    "        x_pad = F.pad(x, (self.padding, self.padding), mode=self.padding_mode)\n",
    "        patches = x_pad.unsqueeze(2).unfold(3, self.kernel_size, 1)\n",
    "\n",
    "        patches = patches.contiguous().view(batch_size, self.in_channels, width, self.kernel_size)\n",
    "\n",
    "        self.patches = patches\n",
    "\n",
    "        \n",
    "\n",
    "        out = torch.einsum('biwk,oik->bow', patches, self.weights) # (biwk) -> (batch_size, in_channels, width, kernel)\n",
    "        out2 = self.einsum_alt(patches, self.weights)\n",
    "        assert torch.allclose(out, out2)#, atol=1e-6, rtol=1e-6)\n",
    "\n",
    "        # Add the bias\n",
    "        if self.bias:\n",
    "            out += self.biases.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "batch_size, width = 1, 5\n",
    "in_channels, out_channels = 1, 1\n",
    "kernel_size, stride = 3, 1\n",
    "\n",
    "x = torch.randn(batch_size, in_channels, width)\n",
    "padding = (kernel_size - 1) // 2\n",
    "\n",
    "padding_mode = 'circular'\n",
    "bias = False\n",
    "\n",
    "# Create conv\n",
    "conv_torch = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, bias=bias, padding=padding, padding_mode=padding_mode)\n",
    "out_true = conv_torch(x)\n",
    "\n",
    "conv = MyConv1d(in_channels, out_channels, kernel_size, stride=stride, bias=bias, padding=padding, padding_mode=padding_mode)\n",
    "conv.weights = nn.Parameter(conv_torch.weight)\n",
    "conv.biases = nn.Parameter(conv_torch.bias)\n",
    "out = conv(x)\n",
    "\n",
    "assert torch.allclose(out_true, out, atol=1e-6, rtol=1e-6)\n",
    "assert torch.allclose(out_true, out) # For some reason, this fails. TODO: Find out why\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 5]),\n",
       " tensor(0.8512, grad_fn=<SelectBackward0>),\n",
       " tensor(0.8512, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = conv.patches\n",
    "w = conv.weights\n",
    "\n",
    "patches = p.permute(0, 2, 1, 3) # # batch_size, width, channels, kernel_size \n",
    "patches = patches.unsqueeze(2) # batch_size, 1, width, channels, kernel_size\n",
    "weigths = w.unsqueeze(0) # 1, out_channels, in_channels, kernel_size\n",
    "out = patches * weigths\n",
    "out = out.sum([3, 4]) # batch_size, out_channels, width, channels\n",
    "out = out.permute(0, 2, 1) # batch_size, out_channels, output_pixels\n",
    "out2 = out\n",
    "\n",
    "p2 = patches[0,0,0]\n",
    "w2 = weigths[0,0]\n",
    "o2 = (p2 * w2).sum()\n",
    "\n",
    "o = torch.einsum('biwk,oik->bow', p, w) # (biwk) -> (batch_size, in_channels, width, kernel)\n",
    "assert o.shape == out_true.shape == out2.shape\n",
    "assert torch.allclose(out_true[0,0,0], o[0,0,0])\n",
    "assert torch.allclose(out_true[0,0,0], out2[0,0,0])\n",
    "assert torch.allclose(out_true[0,0,0], o2)\n",
    "o.shape, o[0,0,0], o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 5, 3]), torch.Size([1, 1, 3]), torch.Size([1, 1, 5]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape, w.shape, o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 0., 0., 0., 0.],\n",
       "        [0., 3., 0., 0., 0.],\n",
       "        [0., 0., 3., 0., 0.],\n",
       "        [0., 0., 0., 3., 0.],\n",
       "        [0., 0., 0., 0., 3.]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cycle(x, i):\n",
    "    return torch.cat([x[i:], x[:i]])\n",
    "\n",
    "eye = torch.eye(5)\n",
    "\n",
    "torch.sum(torch.stack([cycle(eye, i) for i in range(3)]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1305,  0.4368,  0.0000,  0.0000,  0.0532],\n",
       "        [ 0.0532, -0.1305,  0.4368,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0532, -0.1305,  0.4368,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0532, -0.1305,  0.4368],\n",
       "        [ 0.4368,  0.0000,  0.0000,  0.0532, -0.1305]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cycle(x, i):\n",
    "    return torch.cat([x[i:], x[:i]])\n",
    "\n",
    "kernel_weights = torch.zeros(width)\n",
    "kernel_weights[:3] = w.data.squeeze().detach().clone() \n",
    "w3 = torch.stack([cycle(kernel_weights, -i) for i in range(-1, width-1)])\n",
    "w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1688,  0.3409,  0.8512, -0.7342, -0.1783])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('w,kw->k', cycle(x.squeeze(), -2), w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8512, -0.7342, -0.1783, -0.1688,  0.3409])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3 = torch.einsum('w,kw->k', x.squeeze(), w3)\n",
    "o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0532, -0.1305,  0.4368,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5]), torch.Size([5, 5]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.squeeze().shape, w3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eye() received an invalid combination of arguments - got (int, k=int), but expected one of:\n * (int n, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int n, int m, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: eye() received an invalid combination of arguments - got (int, k=int), but expected one of:\n * (int n, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int n, int m, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "torch.eye(3, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (11) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (11) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.diag(torch.ones(10), diagonal=1) ++ torch.diag(torch.ones(10), diagonal=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.]]),\n",
       " tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]])]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.diag(torch.ones(3), diagonal=diagonal) for diagonal in range(-1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiagonal\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: sum(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "torch.sum([torch.diag(torch.ones(10), diagonal=diagonal) for diagonal in range(-10, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.4151, -0.4179,  0.0902],\n",
       "          [-0.4179,  0.0902, -0.7310],\n",
       "          [ 0.0902, -0.7310,  0.2207],\n",
       "          [-0.7310,  0.2207, -2.5049],\n",
       "          [ 0.2207, -2.5049, -0.7893],\n",
       "          [-2.5049, -0.7893,  2.1092],\n",
       "          [-0.7893,  2.1092, -0.6504],\n",
       "          [ 2.1092, -0.6504, -1.3639],\n",
       "          [-0.6504, -1.3639,  0.4151],\n",
       "          [-1.3639,  0.4151, -0.4179]]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]]),\n",
       " torch.Size([4, 3]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[0] == w2, w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3]), torch.Size([4, 3]), torch.Size([]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2.shape, w2.shape, o2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5580,  1.7661, -0.3027],\n",
       "        [ 0.6031,  0.4213,  0.6577],\n",
       "        [-0.6039,  0.0876, -1.5754],\n",
       "        [ 0.0647, -1.4128, -0.7551]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7661, -0.3027,  0.2392, -0.1246, -0.2540,  0.8262, -0.1507,\n",
       "          -1.8762,  0.1936,  0.5580],\n",
       "         [ 0.4213,  0.6577, -0.6326, -2.1045, -0.7548,  1.4206, -0.1227,\n",
       "           0.7463,  0.5374,  0.6031],\n",
       "         [ 0.0876, -1.5754,  0.7371,  0.5767,  1.1310, -0.9098, -0.0612,\n",
       "          -0.9385, -0.2748, -0.6039],\n",
       "         [-1.4128, -0.7551, -1.0314, -0.0925,  0.7355, -0.5328, -0.0863,\n",
       "          -0.5916,  0.9724,  0.0647]],\n",
       "\n",
       "        [[ 0.2188, -1.0770,  0.0059,  1.1088, -0.2786, -1.2330, -0.8237,\n",
       "           2.1114, -0.8140, -0.5056],\n",
       "         [ 0.1743,  1.8318,  1.3409, -0.9931,  0.1322, -0.2115,  0.1997,\n",
       "           0.2212,  0.3497, -0.2611],\n",
       "         [ 0.1338, -0.0476,  0.1128,  0.8461, -0.1283, -1.5674, -0.3284,\n",
       "          -0.3179, -1.0814, -1.2627],\n",
       "         [-0.4925, -1.1542, -0.4834, -0.7194, -0.1469, -0.4030, -0.5062,\n",
       "           0.7469, -1.1123,  0.2723]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 1, 4, 3]), torch.Size([1, 6, 4, 3]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2.shape, w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8715, grad_fn=<SelectBackward0>),\n",
       " tensor(0.8715, grad_fn=<SelectBackward0>),\n",
       " tensor(0.8715, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 10, 3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sympde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
